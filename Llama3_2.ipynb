{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdWft_uNVpxb"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-_WWqYCJVpxc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import re\n",
        "import torch\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "  !pip install unsloth\n",
        "else:\n",
        "  # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "  !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "  !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "  # Upgrade transformers to the required version\n",
        "  !pip install --upgrade \"transformers>=4.53.0\"\n",
        "  !pip install --no-deps unsloth\n",
        "  !pip install pynvml\n",
        "  !pip install outlines\n",
        "\n",
        "from google.colab import files\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from transformers import AutoTokenizer\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "import json\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvLZZO-79JEm"
      },
      "outputs": [],
      "source": [
        "# =================================================================================\n",
        "# I. MODEL AND TOKENIZER INITIALIZATION\n",
        "# =================================================================================\n",
        "\n",
        "# --- 1. Model Configuration ---\n",
        "# Define the maximum number of tokens the model can process.\n",
        "max_seq_length = 8192\n",
        "\n",
        "# --- 2. Load Model and Tokenizer ---\n",
        "# Load the 4-bit quantized Llama-3.2 model using Unsloth for efficiency.\n",
        "# 4-bit loading reduces memory usage.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# --- 3. Configure PEFT with LoRA ---\n",
        "# Apply LoRA for efficient fine-tuning. This updates a small number of parameters\n",
        "# instead of the full model, saving memory and time.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=256,  # LoRA rank.\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention projections\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # Feed-forward layers\n",
        "    ],\n",
        "    lora_alpha=512,  # LoRA scaling factor.\n",
        "    lora_dropout=0,  # LoRA dropout.\n",
        "    bias=\"none\",     # No bias training for optimization.\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Memory-saving technique.\n",
        "    random_state=3407, # Reproducibility seed.\n",
        "    use_rslora=False,  # Rank-Stabilized LoRA (disabled).\n",
        "    loftq_config=None, # LoftQ configuration (disabled).\n",
        ")\n",
        "\n",
        "\n",
        "# =================================================================================\n",
        "# II. DATA PREPARATION AND FORMATTING\n",
        "# =================================================================================\n",
        "\n",
        "# --- 1. Configure Chat Template ---\n",
        "# Set up the tokenizer with the correct chat format for Llama-3.2.\n",
        "print(\"Configuring Llama-3.2 chat template...\")\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template=\"llama-3.2\",\n",
        ")\n",
        "print(\"Template configuration complete.\")\n",
        "\n",
        "# --- 2. Define Formatting Function ---\n",
        "# This function converts each dataset entry (instruction, input, output)\n",
        "# into the Llama-3.2 chat format.\n",
        "def formatting_prompts_func(examples):\n",
        "    \"\"\"Formats a batch of examples into the Llama-3.2 chat structure.\"\"\"\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "\n",
        "    convos = []\n",
        "    # Build the conversation structure for each example\n",
        "    for instruction, user_input, output in zip(instructions, inputs, outputs):\n",
        "        conversation = [\n",
        "            {\"role\": \"system\", \"content\": instruction}, # System instruction\n",
        "            {\"role\": \"user\", \"content\": user_input},    # User input\n",
        "            {\"role\": \"assistant\", \"content\": output},   # Desired model output\n",
        "        ]\n",
        "        convos.append(conversation)\n",
        "\n",
        "    # Apply the template to create formatted text strings\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False) for convo in convos]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# --- 3. Load Datasets ---\n",
        "# Upload and load the two JSONL files for fine-tuning.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Validate that exactly two files were uploaded.\n",
        "if len(uploaded) != 2:\n",
        "    raise ValueError(\"Please upload exactly 2 JSONL files for the two-step fine-tuning.\")\n",
        "\n",
        "# Sort file names for consistent order.\n",
        "file_names = sorted(list(uploaded.keys()))\n",
        "file1, file2 = file_names[0], file_names[1]\n",
        "print(f\"Files loaded:\\nStep 1: {file1}\\nStep 2: {file2}\")\n",
        "\n",
        "# Load each JSONL file into a Dataset object.\n",
        "dataset1 = load_dataset(\"json\", data_files={\"train\": file1}, split=\"train\")\n",
        "dataset2 = load_dataset(\"json\", data_files={\"train\": file2}, split=\"train\")\n",
        "\n",
        "# --- 4. Split Datasets ---\n",
        "# Reserve 5% of the data for validation.\n",
        "split1 = dataset1.train_test_split(test_size=0.05, seed=42)\n",
        "split2 = dataset2.train_test_split(test_size=0.05, seed=42)\n",
        "\n",
        "# --- 5. Apply Formatting to All Datasets ---\n",
        "# Use .map() to efficiently apply the formatting function.\n",
        "print(\"\\nFormatting datasets...\")\n",
        "train_dataset1 = split1[\"train\"].map(formatting_prompts_func, batched=True)\n",
        "val_dataset1 = split1[\"test\"].map(formatting_prompts_func, batched=True)\n",
        "\n",
        "train_dataset2 = split2[\"train\"].map(formatting_prompts_func, batched=True)\n",
        "val_dataset2 = split2[\"test\"].map(formatting_prompts_func, batched=True)\n",
        "\n",
        "print(\"\\nAll datasets have been formatted.\")\n",
        "print(f\"Step 1 - Training samples: {len(train_dataset1)}, Validation samples: {len(val_dataset1)}\")\n",
        "print(f\"Step 2 - Training samples: {len(train_dataset2)}, Validation samples: {len(val_dataset2)}\")\n",
        "\n",
        "# --- 6. Verify Formatted Output ---\n",
        "# Inspect an example to ensure the formatting is correct.\n",
        "print(\"\\n--- Example of a formatted sample from Step 1 ---\")\n",
        "print(train_dataset1[0]['text'])\n",
        "print(\"\\n--- Example of a formatted sample from Step 2 ---\")\n",
        "print(train_dataset2[0]['text'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bFqf6v5f3qJ_"
      },
      "outputs": [],
      "source": [
        "trainer_step1 = SFTTrainer(\n",
        "    model = model,  # The model to be fine-tuned\n",
        "    tokenizer = tokenizer,  # Tokenizer used to preprocess the text data\n",
        "    train_dataset = train_dataset1,  # Training dataset\n",
        "    eval_dataset = val_dataset1,  # Validation dataset\n",
        "    # dataset_text_field = \"text\",  # Specify the dataset field containing text (optional)\n",
        "    max_seq_length = max_seq_length,  # Maximum input sequence length\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),  # Prepares batches for Seq2Seq training\n",
        "    dataset_num_proc = 2,  # Number of processes for dataset preprocessing\n",
        "    packing = False,  # If True, packs multiple short sequences into one for faster training\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4,  # Batch size per device during training\n",
        "        gradient_accumulation_steps = 8,  # Accumulates gradients over multiple steps before updating weights\n",
        "        warmup_steps = 20,  # Number of steps for learning rate warmup\n",
        "        num_train_epochs = 2,  # Number of full passes through the training data\n",
        "        # max_steps = 100,  # Override number of steps (None means run full training)\n",
        "        learning_rate = 1e-5,  # Initial learning rate\n",
        "        fp16 = not is_bfloat16_supported(),  # Use FP16 if BF16 is not supported\n",
        "        bf16 = is_bfloat16_supported(),  # Use BF16 if supported by the GPU\n",
        "        logging_steps = 1,  # Log training metrics every N steps\n",
        "        optim = \"adamw_8bit\",  # Optimizer choice (8-bit AdamW to save memory)\n",
        "        weight_decay = 0.01,  # L2 regularization coefficient\n",
        "        lr_scheduler_type = \"linear\",  # Learning rate schedule type\n",
        "        seed = 3407,  # Random seed for reproducibility\n",
        "        output_dir = \"outputs\",  # Directory to save training outputs\n",
        "        report_to = \"none\",  # Logging integration target (e.g., WandB, TensorBoard)\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Start the training process and store training statistics\n",
        "trainer_stats = trainer_step1.train()\n",
        "\n",
        "# Save the trained model and tokenizer to Google Drive\n",
        "model.save_pretrained(\"/content/drive/MyDrive/CAD_history_model_v15.\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/CAD_history_model_v15.3\")\n",
        "\n",
        "# Alternative save path (commented out)\n",
        "# model.save_pretrained(\"/content/drive/MyDrive/CAD_history_model_v15.5\")\n",
        "# tokenizer.save_pretrained(\"/content/drive/MyDrive/CAD_history_model_v15.5\")\n",
        "\n",
        "# ======================================================\n",
        "#  Memory Release Section\n",
        "# ======================================================\n",
        "\n",
        "# 1. Delete large objects that are no longer needed\n",
        "del model\n",
        "del trainer_step1\n",
        "# Optionally delete other large variables, such as tokenized datasets\n",
        "# del train_dataset1, val_dataset1\n",
        "\n",
        "# 2. Force Python's garbage collector to free up memory\n",
        "gc.collect()\n",
        "\n",
        "# 3. Clear the CUDA memory cache in PyTorch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 4. (Optional) Print GPU memory usage to verify cleanup\n",
        "# Requires 'pynvml' library: !pip install pynvml\n",
        "try:\n",
        "    from pynvml import *\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"‚úÖ GPU memory released. Current memory usage: {info.used//1024**2} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"Unable to check GPU memory, but cleanup executed. Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nM1Cpk7_7Fij"
      },
      "outputs": [],
      "source": [
        "# =================================================================================\n",
        "# SECOND-STEP FINE-TUNING\n",
        "# =================================================================================\n",
        "\n",
        "# --- 1. Load the Model from Step 1 ---\n",
        "# Reload the model with the LoRA adapters that were saved after the first training step.\n",
        "# This allows us to continue training from where we left off.\n",
        "print(\"Loading the model fine-tuned in Step 1...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"/content/drive/MyDrive/CAD_history_model_v15.3\", # Path to the previously saved model\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# --- 2. Configure the SFTTrainer for the Second Dataset ---\n",
        "# We set up a new trainer instance, this time using the second dataset (train_dataset2\n",
        "# and val_dataset2) to continue the fine-tuning process.\n",
        "trainer_step2 = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset2,\n",
        "    eval_dataset=val_dataset2,\n",
        "    max_seq_length=max_seq_length,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
        "    dataset_num_proc=2,\n",
        "    packing=False,\n",
        "\n",
        "    # --- Training Arguments (can be adjusted for the second step if needed) ---\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=8,\n",
        "        warmup_steps=20,\n",
        "        num_train_epochs=2,\n",
        "        learning_rate=1e-5,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# --- 3. Start the Second Training Job ---\n",
        "# This command resumes the fine-tuning process with the second dataset.\n",
        "print(\"\\nStarting second training step...\")\n",
        "trainer_stats = trainer_step2.train()\n",
        "print(\"Second training step completed.\")\n",
        "\n",
        "# --- 4. Save the Final Model and Tokenizer ---\n",
        "# Save the final version of the LoRA adapters and tokenizer to a new directory.\n",
        "print(\"Saving final model and tokenizer...\")\n",
        "model.save_pretrained(\"/content/drive/MyDrive/CAD_history_model_v15.4\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/CAD_history_model_v15.4\")\n",
        "print(\"Final model and tokenizer saved successfully.\")\n",
        "\n",
        "\n",
        "# =================================================================================\n",
        "# FINAL MEMORY CLEANUP\n",
        "# =================================================================================\n",
        "# Release GPU memory after the final training step is complete.\n",
        "\n",
        "# --- 1. Delete Unused Objects ---\n",
        "del model\n",
        "del trainer_step2\n",
        "\n",
        "# --- 2. Force Garbage Collection ---\n",
        "gc.collect()\n",
        "\n",
        "# --- 3. Empty PyTorch's CUDA Cache ---\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# --- 4. (Optional) Check GPU Memory Usage ---\n",
        "try:\n",
        "    from pynvml import *\n",
        "    nvmlInit()\n",
        "    handle = nvmlDeviceGetHandleByIndex(0)\n",
        "    info = nvmlDeviceGetMemoryInfo(handle)\n",
        "    print(f\"‚úÖ Memory released. Current used memory: {info.used//1024**2} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not check memory, but release operations were executed. Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G0C9OzuhUhp"
      },
      "source": [
        "test A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn4hIMIhMsiY"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from google.colab import drive\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "from pprint import pprint\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. ENVIRONMENT SETUP AND MODEL LOADING\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Drive already mounted or mounting failed: {e}\")\n",
        "\n",
        "# --- Load the Fine-Tuned Model ---\n",
        "print(\"\\nLoading the fine-tuned model...\")\n",
        "# NOTE: The user's code points to v15.2, but the last saved model was v15.4.\n",
        "# Please ensure this path points to your final, intended model version.\n",
        "model_path = \"/content/drive/MyDrive/CAD_history_model_v15.4\"\n",
        "model = None  # Initialize variable for error checking.\n",
        "\n",
        "try:\n",
        "    # --- Pre-load Sanity Checks ---\n",
        "    print(f\"Checking model path: {model_path}\")\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(\"Model directory does not exist! Please confirm the path is correct.\")\n",
        "    if not os.path.isdir(model_path):\n",
        "        raise NotADirectoryError(\"The specified path is not a directory.\")\n",
        "\n",
        "    # Check for essential files to ensure the model was saved correctly.\n",
        "    required_files = [\"adapter_config.json\", \"tokenizer_config.json\", \"special_tokens_map.json\"]\n",
        "    missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_path, f))]\n",
        "    if missing_files:\n",
        "        raise FileNotFoundError(\n",
        "            f\"The model directory '{os.path.basename(model_path)}' is missing key files: {', '.join(missing_files)}. \"\n",
        "            \"This usually means the model was not saved correctly or files are corrupted.\"\n",
        "        )\n",
        "    print(\"‚úÖ Model file check passed. Attempting to load...\")\n",
        "\n",
        "    # --- Load Model and Apply Adapters ---\n",
        "    # Step 1: Load the original, un-tuned base model.\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",  # Must point to the original base model.\n",
        "        max_seq_length=8192,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    # Step 2: Apply the saved LoRA adapters to the base model.\n",
        "    print(\"Applying saved LoRA adapters...\")\n",
        "    model.load_adapter(model_path)\n",
        "    print(\"Adapters loaded successfully!\")\n",
        "\n",
        "    # Enable Unsloth's fast inference mode.\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n‚ùå Model loading failed! Error details below:\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    print(\"\\n--- Please check your model files and path based on the error message. ---\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. HELPER FUNCTIONS AND PROMPT DEFINITIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def extract_json_block(raw_text: str):\n",
        "    \"\"\"A robust function to extract the first valid JSON block from raw model output.\"\"\"\n",
        "    if not isinstance(raw_text, str):\n",
        "        return None\n",
        "    # Prioritize matching a Markdown-formatted JSON code block.\n",
        "    json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", raw_text, re.DOTALL)\n",
        "    if json_match:\n",
        "        return json_match.group(1)\n",
        "\n",
        "    # If no Markdown block is found, fall back to finding the first and last curly braces.\n",
        "    first_brace = raw_text.find(\"{\")\n",
        "    last_brace = raw_text.rfind(\"}\")\n",
        "    if first_brace != -1 and last_brace != -1 and last_brace > first_brace:\n",
        "        json_text = raw_text[first_brace : last_brace + 1]\n",
        "        if json_text.strip().startswith(\"{\") and json_text.strip().endswith(\"}\"):\n",
        "            return json_text\n",
        "    return None\n",
        "\n",
        "def generate_response(instruction, user_input, max_tokens=8192):\n",
        "    \"\"\"Generic model invocation function using the Llama-3.2 chat template.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": instruction},\n",
        "        {\"role\": \"user\", \"content\": user_input},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    prompt_len_tokens = inputs['input_ids'].shape[1]\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    response_part = tokenizer.decode(outputs[0][prompt_len_tokens:], skip_special_tokens=True)\n",
        "    return response_part\n",
        "\n",
        "# --- User Inputs ---\n",
        "brief_description_input = \"A stepped cylindrical disc with a central through-hole.\"\n",
        "bbox_input = \"X=2.2,Y=0.7,Z=2.2\"\n",
        "\n",
        "# --- System Prompts for the Two-Step Process ---\n",
        "prompt_step1 = {\n",
        "    \"instruction\": (\n",
        "        \"As a CAD expert, create a procedural build plan from the provided part description and bounding box.\\n\\n\"\n",
        "        \"### Requirements ###\\n\"\n",
        "        \"1.  **Logical Flow**: The plan must be geometrically sound, and the steps (`Sketch`, `Extrude`) must be in a logical sequence to build the part.\\n\"\n",
        "        \"2.  **Bounding Box Adherence**: The final geometry's external dimensions must strictly match the provided bounding box. Use the bounding box to define the primary feature dimensions.\\n\"\n",
        "        \"3.  **Natural Language**: Describe the geometric meaning of operations.\\n\"\n",
        "        \"4.  **Conciseness**: Be direct. Do not include any introductory or concluding text. Your response must start directly with the build plan.\\n\\n\"\n",
        "        \"### Formatting Rules ###\\n\"\n",
        "        \"- **Top-Level Features**: For each feature like `Sketch` or `Extrude`, start a new line with its name in bold (e.g., `**Sketch1**`).\\n\"\n",
        "        \"- **Profile Description**: For a `Sketch`, create an indented bullet point for each `Profile` it contains (e.g., `- **Profile1**`). On the same line, describe its geometry, focusing on loop relationships (inner/outer), shapes, and key dimensions (e.g., \\\"side length X\\\", \\\"radius Y\\\").\\n\"\n",
        "        \"- **Extrude Description**: For an `Extrude` feature, add a colon and the description on the same line as its name. The description must specify the referenced profile, the operation (new body, join, cut), direction, and distance.\"\n",
        "    ),\n",
        "    \"input_template\": (\n",
        "        \"Task: Infer a detailed, precise, and procedural build plan from a brief part description and its bounding box. Provide only the plan itself, strictly following all rules.\\n\\n\"\n",
        "        \"Brief description: {brief_desc}\\n\"\n",
        "        \"Bounding box: {bbox_info}\\n\"\n",
        "    )\n",
        "}\n",
        "\n",
        "prompt_step2 = {\n",
        "    \"instruction\": (\n",
        "        \"Task: Generate a complete, precise, and parametric JSON modeling sequence based on the provided Build Plan. Your output must strictly adhere to the following rules:\\n\"\n",
        "        \"1. **Adherence to Plan**: Strictly and faithfully follow every step, operation, and sequence defined in the 'Build Plan'. Not only the profile itself, but also all the points and curves used within it must be defined. Do not add, omit, or alter the modeling process.\\n\"\n",
        "        \"2. **JSON Structure Integrity**: The root of each JSON file should be 'entities' ONLY. The 'entities' section is a library of all individual modeling features, such as 'Sketch' and 'Extrude'.\\n\"\n",
        "        \"3. **Parameter Integrity**: Provide correct values for all fields in the JSON structure (including numerical values, attributes, and references). No placeholders or missing parameters are permitted. Ensure all entity references are correct.\\n\"\n",
        "        \"4. **Geometric & Numerical Accuracy**: The geometric logic must be flawless: profiles must be closed, constituent curves must connect sequentially (head-to-tail), and the number and type of points/curves defined must match the Build Plan.\\n\"\n",
        "        \"5. **Structural & Syntactic Correctness**: Strictly adhere to the predefined JSON structure, naming conventions, and official terminology. Do not add or remove fields or invent terms. The final output must be a single, valid JSON object.\"\n",
        "    ),\n",
        "    \"input_template\": (\n",
        "        \"Task: Generate a complete, precise, and parametric JSON modeling sequence based on the provided Build Plan. Do not include any comments, explanations, or any other extraneous text.\\n\\n\"\n",
        "        \"--- Build Plan ---\\n\"\n",
        "        \"{build_plan}\"\n",
        "    )\n",
        "}\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. EXECUTE THE TWO-STEP INFERENCE CHAIN\n",
        "# ==============================================================================\n",
        "\n",
        "# Only run inference if the model was loaded successfully.\n",
        "if model:\n",
        "    # --- Step 1: Generate the Modeling Build Plan ---\n",
        "    print(\"=\"*50)\n",
        "    print(\"üöÄ STEP 1: Generating Modeling Build Plan...\")\n",
        "    print(\"=\"*50)\n",
        "    step1_input = prompt_step1[\"input_template\"].format(\n",
        "        brief_desc=brief_description_input,\n",
        "        bbox_info=bbox_input\n",
        "    )\n",
        "    build_plan = generate_response(prompt_step1[\"instruction\"], step1_input, max_tokens=1024)\n",
        "    print(f\"\\n‚úÖ Step 1 Generated Build Plan:\\n---\\n{build_plan}\\n---\")\n",
        "\n",
        "    # --- Step 2: Convert the Build Plan to Full JSON ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üöÄ STEP 2: Converting Build Plan to Full JSON...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    step2_input = prompt_step2[\"input_template\"].format(build_plan=build_plan)\n",
        "    step2_raw_output = generate_response(prompt_step2[\"instruction\"], step2_input, max_tokens=8192)\n",
        "    print(f\"\\n‚úÖ Step 2 Raw Output from Model:\\n---\\n{step2_raw_output}\\n---\")\n",
        "\n",
        "    # --- Final Processing: Extract and Validate JSON ---\n",
        "    final_json_str = extract_json_block(step2_raw_output)\n",
        "\n",
        "    if not final_json_str:\n",
        "        print(\"\\n‚ùå Step 2 Failed: Could not extract a final JSON object from the raw output.\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Step 2 Extracted Final JSON. Checking syntax...\")\n",
        "        try:\n",
        "            # Check if the extracted string is valid JSON.\n",
        "            parsed_json = json.loads(final_json_str)\n",
        "            print(\"---\")\n",
        "            pprint(parsed_json, sort_dicts=False, width=120)\n",
        "            print(\"---\")\n",
        "            print(\"\\n‚úÖ‚úÖ‚úÖ SUCCESS! A syntactically valid JSON object was extracted and parsed. ‚úÖ‚úÖ‚úÖ\")\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            # Catch JSON syntax errors.\n",
        "            print(f\"\\n‚ùå JSON Parsing Failed: The extracted block is not valid JSON. Error: {e}\")\n",
        "            print(\"\\n--- Extracted String was: ---\\n\", final_json_str)\n",
        "\n",
        "else:\n",
        "    print(\"\\nAborting execution because the model was not loaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ==============================================================================\n",
        "# BATCH INFERENCE SCRIPT (with Resume Capability)\n",
        "# ==============================================================================\n",
        "# This script assumes the model, tokenizer, prompts, and helper functions\n",
        "# from the previous cells have already been loaded and defined.\n",
        "\n",
        "# --- 1. Define Output Path on Google Drive ---\n",
        "gdrive_output_path = \"/content/drive/MyDrive/batch_test_results_A.txt\"\n",
        "\n",
        "# --- 2. Resume Logic ---\n",
        "# Check if a results file already exists to skip previously processed items.\n",
        "processed_ids = set()\n",
        "if os.path.exists(gdrive_output_path):\n",
        "    print(f\"‚úÖ Found existing results file: {gdrive_output_path}\")\n",
        "    print(\"Scanning for completed parts to skip...\")\n",
        "    try:\n",
        "        with open(gdrive_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                # Ensure the line is not empty and contains the delimiter.\n",
        "                if line and ';' in line:\n",
        "                    # The part ID is the segment before the first semicolon.\n",
        "                    part_id = line.split(';', 1)[0]\n",
        "                    processed_ids.add(part_id)\n",
        "        print(f\"Found {len(processed_ids)} completed parts. They will be skipped.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Warning: Could not read the existing results file. Starting a fresh run. Error: {e}\")\n",
        "        processed_ids = set() # Reset if reading fails.\n",
        "else:\n",
        "    print(\"No existing results file found. Starting a new run.\")\n",
        "\n",
        "\n",
        "# --- 3. Upload Test Data File ---\n",
        "print(\"\\nüöÄ Please upload your test data file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"\\n‚ùå Operation cancelled, no file was uploaded.\")\n",
        "elif not model:\n",
        "    print(\"\\n‚ùå Model is not loaded. Cannot proceed with batch inference.\")\n",
        "else:\n",
        "    # Get the uploaded file name and content.\n",
        "    test_file_name = next(iter(uploaded))\n",
        "    test_file_content = uploaded[test_file_name].decode('utf-8')\n",
        "    lines = test_file_content.strip().split('\\n')\n",
        "\n",
        "    print(f\"\\nFile '{test_file_name}' uploaded successfully with {len(lines)} lines to process.\")\n",
        "\n",
        "    # --- 4. Main Loop: Process and Write Line-by-Line ---\n",
        "    # Open the file in \"append\" mode to add to it.\n",
        "    with open(gdrive_output_path, \"a\", encoding=\"utf-8\") as output_file:\n",
        "        for line in tqdm(lines, desc=\"Processing parts\"):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # --- 4.1 Parse Data from Each Line ---\n",
        "                # Expected format: PartID;Description;BoundingBox;BuildPlan;InstructionSet\n",
        "                parts = line.split(';')\n",
        "                if len(parts) < 5:\n",
        "                    print(f\"‚ö†Ô∏è Parsing Warning: Skipping malformed line (requires at least 4 semicolons): {line[:100]}...\")\n",
        "                    continue\n",
        "\n",
        "                part_id = parts[0].strip()\n",
        "                # The build plan is the 4th part (index 3).\n",
        "                build_plan_from_file = parts[3].strip()\n",
        "\n",
        "                # --- 4.2 Check if Part Should Be Skipped ---\n",
        "                if part_id in processed_ids:\n",
        "                    continue # Skip to the next loop iteration.\n",
        "\n",
        "                # --- 4.3 Prepare Model Input ---\n",
        "                # This is a key step: restore escaped newlines ('\\\\n') from the file\n",
        "                # back to actual newlines ('\\n') to match the model's training format.\n",
        "                build_plan_for_model = build_plan_from_file.replace('\\\\n', '\\n')\n",
        "\n",
        "                print(f\"\\n‚öôÔ∏è  Processing Part ID: {part_id}...\")\n",
        "                responses = []\n",
        "                # --- 4.4 Call the Model 3 Times for Each Build Plan ---\n",
        "                for i in range(3):\n",
        "                    print(f\"    - Invocation {i+1}/3...\")\n",
        "                    # Use the \"Step 2\" inference logic from the previous cell.\n",
        "                    step2_input = prompt_step2[\"input_template\"].format(build_plan=build_plan_for_model)\n",
        "                    raw_output = generate_response(prompt_step2[\"instruction\"], step2_input, max_tokens=8192)\n",
        "                    json_response = extract_json_block(raw_output)\n",
        "                    final_response = json_response if json_response else raw_output\n",
        "\n",
        "                    # To save in a single line, replace real newlines with escaped ones.\n",
        "                    processed_response = final_response.replace('\\r\\n', '\\\\n').replace('\\n', '\\\\n')\n",
        "                    responses.append(processed_response)\n",
        "\n",
        "                    print(f\"    - Invocation {i+1}/3 complete. JSON extracted: {'‚úÖ' if json_response else '‚ùå'}\")\n",
        "\n",
        "                # --- 4.5 Combine Results and Write to File ---\n",
        "                # Format: \"id;build_plan;response1<|>response2<|>response3\"\n",
        "                # Note: We use the original `build_plan_from_file` to construct the result line.\n",
        "                result_line = f\"{part_id};{build_plan_from_file};{'<|>'.join(responses)}\"\n",
        "\n",
        "                # Write the result line and a newline character.\n",
        "                output_file.write(result_line + \"\\n\")\n",
        "                # Force the buffer to write to disk, ensuring real-time saving.\n",
        "                output_file.flush()\n",
        "\n",
        "                print(\"-\" * 70)\n",
        "                print(f\"‚úÖ Result for Part ID: {part_id} has been saved.\")\n",
        "                print(\"-\" * 70)\n",
        "\n",
        "            except Exception as e:\n",
        "                # Use 'locals()' to safely access part_id if it was assigned.\n",
        "                current_part_id = part_id if 'part_id' in locals() else 'Unknown'\n",
        "                print(f\"‚ùå A critical error occurred while processing Part ID {current_part_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # --- 5. Final Success Message ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"‚úÖ All parts processed!\")\n",
        "    print(f\"‚úÖ Results have been progressively saved to your Google Drive.\")\n",
        "    print(f\"   File Path: {gdrive_output_path}\")\n",
        "    print(\"=\"*50)\n"
      ],
      "metadata": {
        "id": "WTYHe7c9HhPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test B"
      ],
      "metadata": {
        "id": "3q7n5DGJ9V3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from google.colab import drive\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. ENVIRONMENT SETUP AND MODEL LOADING\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Drive already mounted or mounting failed: {e}\")\n",
        "\n",
        "# --- Load the Fine-Tuned Model ---\n",
        "print(\"\\nLoading the fine-tuned model...\")\n",
        "model_path = \"/content/drive/MyDrive/CAD_history_model_v15.4\"\n",
        "model = None  # Initialize variable for error checking.\n",
        "\n",
        "try:\n",
        "    # --- Pre-load Sanity Checks ---\n",
        "    print(f\"Checking model path: {model_path}\")\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(\"Model directory does not exist! Please confirm the path is correct.\")\n",
        "    if not os.path.isdir(model_path):\n",
        "        raise NotADirectoryError(\"The specified path is not a directory.\")\n",
        "\n",
        "    # Check for essential files.\n",
        "    required_files = [\"adapter_config.json\", \"tokenizer_config.json\", \"special_tokens_map.json\"]\n",
        "    missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_path, f))]\n",
        "    if missing_files:\n",
        "        raise FileNotFoundError(\n",
        "            f\"The model directory '{os.path.basename(model_path)}' is missing key files: {', '.join(missing_files)}. \"\n",
        "            \"This usually means the model was not saved correctly.\"\n",
        "        )\n",
        "    print(\"‚úÖ Model file check passed. Attempting to load...\")\n",
        "\n",
        "    # --- Load Model and Apply Adapters ---\n",
        "    # Step 1: Load the original base model.\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "        max_seq_length=8192,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    # Step 2: Apply the saved LoRA adapters.\n",
        "    print(\"Applying saved LoRA adapters...\")\n",
        "    model.load_adapter(model_path)\n",
        "    print(\"Adapters loaded successfully!\")\n",
        "\n",
        "    # Enable Unsloth's fast inference mode.\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n‚ùå Model loading failed! Error details below:\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    print(\"\\n--- Please check your model files and path based on the error message. ---\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. HELPER FUNCTIONS AND PROMPT DEFINITIONS\n",
        "# ==============================================================================\n",
        "\n",
        "def extract_json_block(raw_text: str):\n",
        "    \"\"\"A robust function to extract the first valid JSON block from raw model output.\"\"\"\n",
        "    if not isinstance(raw_text, str):\n",
        "        return None\n",
        "    # Prioritize matching a Markdown-formatted JSON code block.\n",
        "    json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", raw_text, re.DOTALL)\n",
        "    if json_match:\n",
        "        return json_match.group(1)\n",
        "\n",
        "    # If no Markdown block is found, fall back to finding the first and last curly braces.\n",
        "    first_brace = raw_text.find(\"{\")\n",
        "    last_brace = raw_text.rfind(\"}\")\n",
        "    if first_brace != -1 and last_brace != -1 and last_brace > first_brace:\n",
        "        json_text = raw_text[first_brace : last_brace + 1]\n",
        "        if json_text.strip().startswith(\"{\") and json_text.strip().endswith(\"}\"):\n",
        "            return json_text\n",
        "    return None\n",
        "\n",
        "def generate_response(instruction, user_input, max_tokens=1024):\n",
        "    \"\"\"Generic model invocation function using the Llama-3.2 chat template.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": instruction},\n",
        "        {\"role\": \"user\", \"content\": user_input},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    prompt_len_tokens = inputs['input_ids'].shape[1]\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.2,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    response_part = tokenizer.decode(outputs[0][prompt_len_tokens:], skip_special_tokens=True)\n",
        "    return response_part\n",
        "\n",
        "# --- User Inputs ---\n",
        "brief_description_input = \"A stepped cylindrical disc with a central through-hole.\"\n",
        "bbox_input = \"X=2.2,Y=0.7,Z=2.2\"\n",
        "\n",
        "# --- System Prompts for the Two-Step Process ---\n",
        "prompt_step1 = {\n",
        "    \"instruction\": (\n",
        "        \"As a CAD expert, create a procedural build plan from the provided part description and bounding box.\\n\\n\"\n",
        "        \"### Requirements ###\\n\"\n",
        "        \"1.  **Logical Flow**: The plan must be geometrically sound, and the steps (`Sketch`, `Extrude`) must be in a logical sequence to build the part.\\n\"\n",
        "        \"2.  **Bounding Box Adherence**: The final geometry's external dimensions must strictly match the provided bounding box. Use the bounding box to define the primary feature dimensions.\\n\"\n",
        "        \"3.  **Natural Language**: Describe the geometric meaning of operations.\\n\"\n",
        "        \"4.  **Conciseness**: Be direct. Do not include any introductory or concluding text. Your response must start directly with the build plan.\\n\\n\"\n",
        "        \"### Formatting Rules ###\\n\"\n",
        "        \"- **Top-Level Features**: For each feature like `Sketch` or `Extrude`, start a new line with its name in bold (e.g., `**Sketch1**`).\\n\"\n",
        "        \"- **Profile Description**: For a `Sketch`, create an indented bullet point for each `Profile` it contains (e.g., `- **Profile1**`). On the same line, describe its geometry, focusing on loop relationships (inner/outer), shapes, and key dimensions (e.g., \\\"side length X\\\", \\\"radius Y\\\").\\n\"\n",
        "        \"- **Extrude Description**: For an `Extrude` feature, add a colon and the description on the same line as its name. The description must specify the referenced profile, the operation (new body, join, cut), direction, and distance.\"\n",
        "    ),\n",
        "    \"input_template\": (\n",
        "        \"Task: Infer a detailed, precise, and procedural build plan from a brief part description and its bounding box. Provide only the plan itself, strictly following all rules.\\n\\n\"\n",
        "        \"Brief description: {brief_desc}\\n\"\n",
        "        \"Bounding box: {bbox_info}\\n\"\n",
        "    )\n",
        "}\n",
        "\n",
        "prompt_step2 = {\n",
        "    \"instruction\": (\n",
        "        \"\"\"\n",
        "Task: Convert the CAD Build Plan into a structured, machine-readable command set.\n",
        "\n",
        "### Rules ###\n",
        "1.  **Direct Translation**: Faithfully translate every step from the Build Plan into its corresponding command format.\n",
        "2.  **Geometric Continuity**: The multiple curve segments that form a profile must be connected head-to-tail by their coordinates.\n",
        "3.  **Strict Syntax**: Adhere strictly to the command syntax, parameters, and ordering defined in the reference below.\n",
        "4.  **Completeness**: Ensure every detail from the Build Plan (dimensions, references, operations) is represented in the final instruction set.\n",
        "5.  **Conciseness**: Your response must contain only the command set. Do not include any other text, comments, or explanations.\n",
        "\n",
        "### Command Reference ###\n",
        "- **`S (sketch_id)`**: Marks the beginning of a new sketch. `sketch_id` is a natural number (1, 2, ...).\n",
        "- **`P (profile_id)`**: Marks the beginning of a new profile within a sketch. `profile_id` is a natural number.\n",
        "- **`O (is_outer)`**: Defines the loop type for the current profile. `is_outer` is `true` (outer loop) or `false` (inner loop).\n",
        "- **`L (x1, y1, x2, y2)`**: Defines a straight line using absolute start (x1, y1) and end (x2, y2) coordinates.\n",
        "- **`A (x1, y1, x2, y2, cx, cy, rvx, rvy, r, sa, ea)`**: Defines an arc with start/end points, center, reference vector, radius, and start/end angles.\n",
        "- **`C (cx, cy, r)`**: Defines a full circle with center (cx, cy) and radius (r).\n",
        "- **`T (ox, oy, oz, xx, xy, xz, yx, yy, yz, zx, zy, zz)`**: Defines the 3D spatial pose (origin and axis vectors) of the current sketch.\n",
        "- **`E (profile_id, sketch_id, operation, type, d1, d2)`**: Defines an extrusion feature.\n",
        "        \"\"\"\n",
        "    ),\n",
        "    \"input_template\": (\n",
        "        \"Task: Convert the CAD Build Plan into a structured, machine-readable command set.\\n\\n\"\n",
        "        \"--- Build Plan ---\\n\"\n",
        "        \"{build_plan}\"\n",
        "    )\n",
        "}\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. EXECUTE THE TWO-STEP INFERENCE CHAIN\n",
        "# ==============================================================================\n",
        "\n",
        "# Only run inference if the model was loaded successfully.\n",
        "if model:\n",
        "    # --- Step 1: Generate the Modeling Build Plan ---\n",
        "    print(\"=\"*50)\n",
        "    print(\"üöÄ STEP 1: Generating Modeling Build Plan...\")\n",
        "    print(\"=\"*50)\n",
        "    step1_input = prompt_step1[\"input_template\"].format(\n",
        "        brief_desc=brief_description_input,\n",
        "        bbox_info=bbox_input\n",
        "    )\n",
        "    build_plan = generate_response(prompt_step1[\"instruction\"], step1_input, max_tokens=1024)\n",
        "    print(f\"\\n‚úÖ Step 1 Generated Build Plan:\\n---\\n{build_plan}\\n---\")\n",
        "\n",
        "    # --- Step 2: Convert the Build Plan to a Command Set ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"üöÄ STEP 2: Converting Build Plan to Command Set...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    step2_input = prompt_step2[\"input_template\"].format(build_plan=build_plan)\n",
        "    step2_raw_output = generate_response(prompt_step2[\"instruction\"], step2_input, max_tokens=8192)\n",
        "\n",
        "    # --- Final Processing: Display the raw model output directly ---\n",
        "    print(f\"\\n‚úÖ Step 2 Raw Output from Model (Command Set):\\n---\\n{step2_raw_output}\\n---\")\n",
        "    print(\"\\n‚úÖ‚úÖ‚úÖ SUCCESS! The two-step process is complete. ‚úÖ‚úÖ‚úÖ\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nAborting execution because the model was not loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "zxynnAki9VYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. BATCH INFERENCE SCRIPT (Full Two-Step Process)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. Define Output Path on Google Drive ---\n",
        "gdrive_output_path = \"/content/drive/MyDrive/batch_test_results_B.txt\"\n",
        "\n",
        "# --- 2. Resume Logic ---\n",
        "processed_ids = set()\n",
        "if os.path.exists(gdrive_output_path):\n",
        "    print(f\"‚úÖ Found existing results file: {gdrive_output_path}\")\n",
        "    print(\"Scanning for completed parts to skip...\")\n",
        "    try:\n",
        "        with open(gdrive_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line and ';' in line:\n",
        "                    part_id = line.split(';', 1)[0]\n",
        "                    processed_ids.add(part_id)\n",
        "        print(f\"Found {len(processed_ids)} completed parts. They will be skipped.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Warning: Could not read the existing results file. Starting a fresh run. Error: {e}\")\n",
        "        processed_ids = set()\n",
        "else:\n",
        "    print(\"No existing results file found. Starting a new run.\")\n",
        "\n",
        "\n",
        "# --- 3. Upload Test Data File ---\n",
        "print(\"\\nüöÄ Please upload your test data file (Format: PartID;Description;BBox;BuildPlan;StandardCommandSet)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"\\n‚ùå Operation cancelled, no file was uploaded.\")\n",
        "elif not model:\n",
        "    print(\"\\n‚ùå Model is not loaded. Cannot proceed with batch inference.\")\n",
        "else:\n",
        "    test_file_name = next(iter(uploaded))\n",
        "    test_file_content = uploaded[test_file_name].decode('utf-8')\n",
        "    lines = test_file_content.strip().split('\\n')\n",
        "\n",
        "    print(f\"\\nFile '{test_file_name}' uploaded successfully with {len(lines)} lines to process.\")\n",
        "\n",
        "    # --- 4. Main Loop: Process and Write Line-by-Line ---\n",
        "    with open(gdrive_output_path, \"a\", encoding=\"utf-8\") as output_file:\n",
        "        for line in tqdm(lines, desc=\"Processing parts\"):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            part_id = \"Unknown\"  # Initialize for error handling.\n",
        "            try:\n",
        "                # --- 4.1 Parse Data from Each Line ---\n",
        "                parts = line.split(';')\n",
        "                if len(parts) < 5:\n",
        "                    print(f\"‚ö†Ô∏è Parsing Warning: Skipping malformed line (requires 5 semicolon-separated parts): {line[:100]}...\")\n",
        "                    continue\n",
        "\n",
        "                part_id = parts[0].strip()\n",
        "                brief_description_input = parts[1].strip()\n",
        "                bbox_input = parts[2].strip()\n",
        "                # The standard command set from the file is the 5th part (index 4).\n",
        "                standard_command_set_from_file = parts[4].strip()\n",
        "\n",
        "                # --- 4.2 Check if Part Should Be Skipped ---\n",
        "                if part_id in processed_ids:\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\n‚öôÔ∏è  Processing Part ID: {part_id}...\")\n",
        "\n",
        "                # --- 4.3 [Step 1] Generate the Build Plan ---\n",
        "                print(\"   - STEP 1: Generating Build Plan...\")\n",
        "                step1_input = prompt_step1[\"input_template\"].format(\n",
        "                    brief_desc=brief_description_input,\n",
        "                    bbox_info=bbox_input\n",
        "                )\n",
        "                build_plan = generate_response(prompt_step1[\"instruction\"], step1_input, max_tokens=1024)\n",
        "                print(\"   - STEP 1: Plan Generated.\")\n",
        "\n",
        "                # --- 4.4 [Step 2] Convert Plan to Command Set (3 runs) ---\n",
        "                print(\"   - STEP 2: Converting Plan to Command Set (3 runs)...\")\n",
        "                newly_generated_command_sets = []\n",
        "                for i in range(3):\n",
        "                    print(f\"     - Run {i+1}/3...\")\n",
        "                    step2_input = prompt_step2[\"input_template\"].format(build_plan=build_plan)\n",
        "                    raw_output = generate_response(prompt_step2[\"instruction\"], step2_input, max_tokens=8192)\n",
        "\n",
        "                    # Replace newlines with escaped characters for single-line storage.\n",
        "                    processed_response = raw_output.replace('\\r\\n', '\\\\n').replace('\\n', '\\\\n')\n",
        "                    newly_generated_command_sets.append(processed_response)\n",
        "                    print(f\"     - Run {i+1}/3 finished.\")\n",
        "\n",
        "                # --- 4.5 Combine Results and Write to File ---\n",
        "                # Output format: \"id;standard_command_set_from_file;new_set_1<|>new_set_2<|>new_set_3\"\n",
        "                all_new_command_set_results = \"<|>\".join(newly_generated_command_sets)\n",
        "                result_line = f\"{part_id};{standard_command_set_from_file};{all_new_command_set_results}\"\n",
        "\n",
        "                output_file.write(result_line + \"\\n\")\n",
        "                output_file.flush() # Ensure real-time saving.\n",
        "\n",
        "                print(\"-\" * 70)\n",
        "                print(f\"‚úÖ Full two-step result for Part ID: {part_id} has been saved.\")\n",
        "                print(\"-\" * 70)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå A critical error occurred while processing Part ID {part_id}: {e}\")\n",
        "                error_line = f\"{part_id};ERROR;{str(e).replace(';', ',')}\\n\"\n",
        "                output_file.write(error_line)\n",
        "                output_file.flush()\n",
        "                continue\n",
        "\n",
        "    # --- 5. Final Success Message ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ All parts processed!\")\n",
        "    print(\"‚úÖ Results have been progressively saved to your Google Drive.\")\n",
        "    print(f\"   File Path: {gdrive_output_path}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "id": "ZnNYjLG_-qPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test C"
      ],
      "metadata": {
        "id": "gaQ56DZL8qr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import re\n",
        "from google.colab import drive\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "from pprint import pprint\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. ENVIRONMENT SETUP AND MODEL LOADING\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Drive already mounted or mounting failed: {e}\")\n",
        "\n",
        "# --- Load the Fine-Tuned Model ---\n",
        "print(\"\\nLoading the fine-tuned model...\")\n",
        "model_path = \"/content/drive/MyDrive/CAD_history_model_v15.5\"\n",
        "model = None  # Initialize variable for error checking.\n",
        "\n",
        "try:\n",
        "    # --- Pre-load Sanity Checks ---\n",
        "    print(f\"Checking model path: {model_path}\")\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(\"Model directory does not exist! Please confirm the path is correct.\")\n",
        "    if not os.path.isdir(model_path):\n",
        "        raise NotADirectoryError(\"The specified path is not a directory.\")\n",
        "\n",
        "    # Check for essential files.\n",
        "    required_files = [\"adapter_config.json\", \"tokenizer_config.json\", \"special_tokens_map.json\"]\n",
        "    missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_path, f))]\n",
        "    if missing_files:\n",
        "        raise FileNotFoundError(\n",
        "            f\"The model directory '{os.path.basename(model_path)}' is missing key files: {', '.join(missing_files)}. \"\n",
        "            \"This usually means the model was not saved correctly.\"\n",
        "        )\n",
        "    print(\"‚úÖ Model file check passed. Attempting to load...\")\n",
        "\n",
        "    # --- Load Model and Apply Adapters ---\n",
        "    # Step 1: Load the original base model.\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "        max_seq_length=8192,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    # Step 2: Apply the saved LoRA adapters.\n",
        "    print(\"Applying saved LoRA adapters...\")\n",
        "    model.load_adapter(model_path)\n",
        "    print(\"Adapters loaded successfully!\")\n",
        "\n",
        "    # Enable Unsloth's fast inference mode.\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n‚ùå Model loading failed! Error details below:\")\n",
        "    print(f\"Error Type: {type(e).__name__}\")\n",
        "    print(f\"Error Message: {e}\")\n",
        "    print(\"\\n--- Please check your model files and path based on the error message. ---\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. HELPER FUNCTIONS AND PROMPT DEFINITION\n",
        "# ==============================================================================\n",
        "\n",
        "def extract_json_block(raw_text: str):\n",
        "    \"\"\"A robust function to extract the first valid JSON block from raw model output.\"\"\"\n",
        "    if not isinstance(raw_text, str):\n",
        "        return None\n",
        "    # Prioritize matching a Markdown-formatted JSON code block.\n",
        "    json_match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", raw_text, re.DOTALL)\n",
        "    if json_match:\n",
        "        return json_match.group(1)\n",
        "\n",
        "    # If no Markdown block is found, fall back to finding the first and last curly braces.\n",
        "    first_brace = raw_text.find(\"{\")\n",
        "    last_brace = raw_text.rfind(\"}\")\n",
        "    if first_brace != -1 and last_brace != -1 and last_brace > first_brace:\n",
        "        json_text = raw_text[first_brace : last_brace + 1]\n",
        "        if json_text.strip().startswith(\"{\") and json_text.strip().endswith(\"}\"):\n",
        "            return json_text\n",
        "    return None\n",
        "\n",
        "def generate_response(instruction, user_input, max_tokens=8192):\n",
        "    \"\"\"Generic model invocation function using the Llama-3.2 chat template.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": instruction},\n",
        "        {\"role\": \"user\", \"content\": user_input},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    prompt_len_tokens = inputs['input_ids'].shape[1]\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.25,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    response_part = tokenizer.decode(outputs[0][prompt_len_tokens:], skip_special_tokens=True)\n",
        "    return response_part\n",
        "\n",
        "# --- User Inputs ---\n",
        "brief_description_input = \"A stepped cylindrical disc with a central through-hole.\"\n",
        "bbox_input = \"X=2.2,Y=0.7,Z=2.2\"\n",
        "\n",
        "\n",
        "# --- New Single-Step System Prompt ---\n",
        "prompt_single_step = {\n",
        "    \"instruction\": (\"\"\"\n",
        "Task: Generate a structured, machine-readable command set directly from a CAD part description and its bounding box.\n",
        "\n",
        "### Requirements ###\n",
        "1.  **Direct Generation**: Infer the geometric steps and translate them directly into the specified command format.\n",
        "2.  **Bounding Box Adherence**: The commands must produce a model whose final dimensions strictly match the provided bounding box.\n",
        "3.  **Geometric Continuity**: The multiple curve segments that form a profile must be connected head-to-tail by their coordinates.\n",
        "4.  **Strict Syntax**: Adhere strictly to the command syntax, parameters, and ordering defined in the reference below.\n",
        "5.  **Conciseness**: Your response must contain only the command set. Do not include any other text, comments, or explanations.\n",
        "\n",
        "### Command Reference ###\n",
        "- **`S (sketch_id)`**: Marks the beginning of a new sketch. `sketch_id` is a natural number (1, 2, ...).\n",
        "- **`P (profile_id)`**: Marks the beginning of a new profile within a sketch. `profile_id` is a natural number.\n",
        "- **`O (is_outer)`**: Defines the loop type for the current profile. `is_outer` is `true` (outer loop) or `false` (inner loop).\n",
        "- **`L (x1, y1, x2, y2)`**: Defines a straight line using absolute start (x1, y1) and end (x2, y2) coordinates.\n",
        "- **`A (x1, y1, x2, y2, cx, cy, rvx, rvy, r, sa, ea)`**: Defines an arc with start/end points, center, reference vector, radius, and start/end angles.\n",
        "- **`C (cx, cy, r)`**: Defines a full circle with center (cx, cy) and radius (r).\n",
        "- **`T (ox, oy, oz, xx, xy, xz, yx, yy, yz, zx, zy, zz)`**: Defines the 3D spatial pose (origin and axis vectors) of the current sketch.\n",
        "- **`E (profile_id, sketch_id, operation, type, d1, d2)`**: Defines an extrusion feature.\n",
        "        \"\"\"\n",
        "    ),\n",
        "    \"input_template\": (\n",
        "        \"Task: Infer the geometric modeling steps and generate a structured, machine-readable command set from the following part description and bounding box. Provide only the command set, strictly following all rules.\\n\\n\"\n",
        "        \"Brief description: {brief_desc}\\n\"\n",
        "        \"Bounding box: {bbox_info}\\n\"\n",
        "    )\n",
        "}\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. EXECUTE THE COMBINED SINGLE-STEP INFERENCE\n",
        "# ==============================================================================\n",
        "\n",
        "# Only run inference if the model was loaded successfully.\n",
        "if model:\n",
        "    print(\"=\"*50)\n",
        "    print(\"üöÄ EXECUTING SINGLE-STEP INFERENCE...\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # --- Combine all information into a single input to generate the final output ---\n",
        "    final_input = prompt_single_step[\"input_template\"].format(\n",
        "        brief_desc=brief_description_input,\n",
        "        bbox_info=bbox_input\n",
        "    )\n",
        "    raw_output = generate_response(prompt_single_step[\"instruction\"], final_input, max_tokens=1024)\n",
        "    print(f\"\\n‚úÖ Raw Output from Model:\\n---\\n{raw_output}\\n---\")\n",
        "\n",
        "    # --- Final Processing: Extract and check JSON syntax ---\n",
        "    # NOTE: The prompt asks for a \"command set\", but the code below attempts to\n",
        "    # extract and parse a JSON object. This may fail if the model produces the\n",
        "    # command set format as requested.\n",
        "    final_json_str = extract_json_block(raw_output)\n",
        "\n",
        "    if not final_json_str:\n",
        "        print(\"\\n‚ùå FAILED: Could not extract a final JSON object from the raw output.\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Extracted Final JSON. Checking syntax...\")\n",
        "        try:\n",
        "            # Check if the extracted string is valid JSON.\n",
        "            parsed_json = json.loads(final_json_str)\n",
        "            print(\"---\")\n",
        "            pprint(parsed_json, sort_dicts=False, width=120)\n",
        "            print(\"---\")\n",
        "            print(\"\\n‚úÖ‚úÖ‚úÖ SUCCESS! A syntactically valid JSON object was extracted and parsed. ‚úÖ‚úÖ‚úÖ\")\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            # Catch JSON syntax errors.\n",
        "            print(f\"\\n‚ùå JSON Parsing Failed: The extracted block is not valid JSON. Error: {e}\")\n",
        "            print(\"\\n--- Extracted String was: ---\\n\", final_json_str)\n",
        "\n",
        "else:\n",
        "    print(\"\\nAborting execution because the model was not loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "w1lvtiLg8twP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. BATCH INFERENCE SCRIPT (Single-Step Process)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- 1. Define Output Path on Google Drive ---\n",
        "gdrive_output_path = \"/content/drive/MyDrive/batch_test_results_C.txt\"\n",
        "\n",
        "# --- 2. Resume Logic ---\n",
        "processed_ids = set()\n",
        "if os.path.exists(gdrive_output_path):\n",
        "    print(f\"‚úÖ Found existing results file: {gdrive_output_path}\")\n",
        "    print(\"Scanning for completed parts to skip...\")\n",
        "    try:\n",
        "        with open(gdrive_output_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line and ';' in line:\n",
        "                    part_id = line.split(';', 1)[0]\n",
        "                    processed_ids.add(part_id)\n",
        "        print(f\"Found {len(processed_ids)} completed parts. They will be skipped.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Warning: Could not read the existing results file. Starting a fresh run. Error: {e}\")\n",
        "        processed_ids = set()\n",
        "else:\n",
        "    print(\"No existing results file found. Starting a new run.\")\n",
        "\n",
        "\n",
        "# --- 3. Upload Test Data File ---\n",
        "print(\"\\nüöÄ Please upload your test data file...\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "if not uploaded:\n",
        "    print(\"\\n‚ùå Operation cancelled, no file was uploaded.\")\n",
        "elif not model:\n",
        "    print(\"\\n‚ùå Model is not loaded. Cannot proceed with batch inference.\")\n",
        "else:\n",
        "    test_file_name = next(iter(uploaded))\n",
        "    test_file_content = uploaded[test_file_name].decode('utf-8')\n",
        "    lines = test_file_content.strip().split('\\n')\n",
        "\n",
        "    print(f\"\\nFile '{test_file_name}' uploaded successfully with {len(lines)} lines to process.\")\n",
        "\n",
        "    # --- 4. Main Loop: Process and Write Line-by-Line ---\n",
        "    with open(gdrive_output_path, \"a\", encoding=\"utf-8\") as output_file:\n",
        "        for line in tqdm(lines, desc=\"Processing parts\"):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "\n",
        "            part_id = \"Unknown\" # Initialize for error handling\n",
        "            try:\n",
        "                # --- 4.1 Parse Data from Each Line ---\n",
        "                # Format: PartID;Description;BoundingBox;BuildPlan;CommandSet\n",
        "                parts = line.split(';')\n",
        "                if len(parts) < 5:\n",
        "                    print(f\"‚ö†Ô∏è Parsing Warning: Skipping malformed line (requires at least 5 parts): {line[:100]}...\")\n",
        "                    continue\n",
        "\n",
        "                part_id = parts[0].strip()\n",
        "                brief_desc_from_file = parts[1].strip() # Description is the 2nd part\n",
        "                bbox_from_file = parts[2].strip()       # Bounding box is the 3rd part\n",
        "                command_from_file = parts[4].strip()    # Command set is the 5th part\n",
        "\n",
        "                # --- 4.2 Check if Part Should Be Skipped ---\n",
        "                if part_id in processed_ids:\n",
        "                    continue\n",
        "\n",
        "                # --- 4.3 Prepare Model Input and Generate Responses ---\n",
        "                print(f\"\\n‚öôÔ∏è  Processing Part ID: {part_id}...\")\n",
        "                responses = []\n",
        "                # --- 4.4 Call the model 3 times for the same input ---\n",
        "                for i in range(3):\n",
        "                    print(f\"    - Invocation {i+1}/3...\")\n",
        "                    # Use the single-step prompt with data from the file\n",
        "                    step_input = prompt_single_step[\"input_template\"].format(\n",
        "                        brief_desc=brief_desc_from_file,\n",
        "                        bbox_info=bbox_from_file\n",
        "                    )\n",
        "                    raw_output = generate_response(prompt_single_step[\"instruction\"], step_input, max_tokens=1024)\n",
        "\n",
        "                    # The raw output is the final response for this workflow\n",
        "                    final_response = raw_output\n",
        "\n",
        "                    # Replace newlines with escaped characters for single-line storage\n",
        "                    processed_response = final_response.replace('\\r\\n', '\\\\n').replace('\\n', '\\\\n')\n",
        "                    responses.append(processed_response)\n",
        "                    print(f\"    - Invocation {i+1}/3 complete.\")\n",
        "\n",
        "                # --- 4.5 Combine Results and Write to File ---\n",
        "                # Format: \"id;command_set_from_file;response1<|>response2<|>response3\"\n",
        "                result_line = f\"{part_id};{command_from_file};{'<|>'.join(responses)}\"\n",
        "\n",
        "                output_file.write(result_line + \"\\n\")\n",
        "                output_file.flush() # Ensure real-time saving\n",
        "\n",
        "                print(\"-\" * 70)\n",
        "                print(f\"‚úÖ Result for Part ID: {part_id} has been saved.\")\n",
        "                print(\"-\" * 70)\n",
        "\n",
        "            except Exception as e:\n",
        "                current_part_id = part_id if 'part_id' in locals() else 'Unknown'\n",
        "                print(f\"‚ùå A critical error occurred while processing Part ID {current_part_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # --- 5. Final Success Message ---\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úÖ All parts processed!\")\n",
        "    print(\"‚úÖ Results have been progressively saved to your Google Drive.\")\n",
        "    print(f\"   File Path: {gdrive_output_path}\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "id": "kbKyfiAw9K9U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}